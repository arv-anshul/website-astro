---
pubDate: 2025-01-01
icon: lucide:notebook-pen
title: January Journal
description: Weekly Journal by ARV of January 2025
categories: [journal, january]
---

## Week 01 Journal

1. Learning **how to fine-tune BERT model using PyTorch** from HuggingFace example.
   - **Sentiment Classification:**
     [transformers_sentiment_wandb.ipynb](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_sentiment_wandb.ipynb)
   - **Multiclass Classification:**
     [transformers_multiclass_classification.ipynb](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb)
2. Planning to leverage the power of **Keras 3** which is [written in their docs](https://keras.io/keras_3/).
   - Its hard (or not possible) because HF's `transformers` library doesn't support it but I have checked that
     `keras_hub` library has it's own BERT model there which I can use, I think.
3. **Will explore more in next week**:
   - [x] How to fine-tune BERT using PyTorch + HF Transformers?
   - [ ] How to fine-tune BERT using Tensorflow + HF Transformers?
   - [x] ~~How to fine-tune BERT using Keras3 + PyTorch + HF Transformers only?~~ _(NOT POSSIBLE)_
   - [x] What are these terms like SPDA, PEFT, LORA and more for fine-tuning purpose.

## Week 02 Journal

1. Explored new method to finetune BERT model using `transformers.TrainingArguments` and `transformers.Trainer` classes.
2. Wrote a [ref page](https://arv-anshul.github.io/ref/deep-learning/finetune-transformers/) where important articles
   are listed for finetuning BERT model.
3. Commiting all the finetuning codes in the form of `python` script and `marimo` notebook in
   [`@arv-anshul/notebooks`](https://github.com/arv-anshul/notebooks) repo.
4. Explored PEFT methods like LoRA. Tried to finetune BERT model using them.
5. Found a way to format my `mkdocs` docs. See `@astral-sh/ruff` repo to know more about formatting `mkdocs` docs.
   - [`.pre-commit-config.yaml`](https://github.com/astral-sh/ruff/blob/main/.pre-commit-config.yaml)
   - [`.markdownlint.yaml`](https://github.com/astral-sh/ruff/blob/main/.markdownlint.yaml)
6. Planning to migrate all `/ref` pages to `/blog` because these aren't any different from them. Also, I don't write
   blogs because I feel the articles are better fit for `/ref` pages which reduced the usages of `/blog` pages.
   - Migration maybe broke some links that I will fix later.

## Week 03 Journal

1. Finally, committed the **Journal Summarizer** as [marimo](https://marimo.io) app for this diary repo.
2. :star: Migrated `/ref` pages to `/blog` in website.
3. Also tried to format markdown files in website's repo using
   [mdformat-mkdocs](https://github.com/kyleking/mdformat-mkdocs) but it's not acceptable for me.
4. Raised [issue in mdformat-mkdocs](https://github.com/KyleKing/mdformat-mkdocs/issues/45) repo.
5. Worked on creating my resume (CV) using LaTeX (`.tex` format). And tried to manage it with python script.

## Week 04 Journal

1. More work on resume (CV) management. Explored LateX and `pdflatex` like tools for resume management.
2. New repo [@arv-anshul/hockey](https://github.com/arv-anshul/hockey). Scrapes data related to Hockey from altiusrt.com
   websites using scrapy framework. And publish the data on
   [Kaggle](https://kaggle.com/datasets/arvanshul/hockey-india-league-2025).
3. Wrote a [Kaggle notebook](https://kaggle.com/code/arvanshul/reddit-sentiment-keras3) on sentiment classification on
   Reddit comments dataset using Keras.
4. Exploring [`deepseek-r1`](https://github.com/deepseek-ai/DeepSeek-R1) distill models with
   [`smolagents`](https://huggingface.co/docs/smolagents) for nlp-to-sql tasks.
5. Hectic exploration of LaTeX tool with [Podman](https://podman.io). _(literally, very hectic)_

## Week 05 Journal

1. Created @Suraj's college PPT on **Introduction to Systems of Human Body** at his place.
2. New repo [@arv-anshul/resume](https://github.com/arv-anshul/resume).
3. @Nitish sir (@CampusX) announced **Gen AI Course** for free on YouTube.
4. Joined Kaggle Competition S05E01.
5. Feeling clueless with next step around anything (project, learning).
